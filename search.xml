<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Pytorch入门</title>
      <link href="/read/Pytorch%E5%85%A5%E9%97%A8-0/"/>
      <url>/read/Pytorch%E5%85%A5%E9%97%A8-0/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch-Tutorial"><a href="#Pytorch-Tutorial" class="headerlink" title="Pytorch Tutorial"></a>Pytorch Tutorial</h1><h2 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#print(torch.__version__)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#print(torch.cuda.is_available())</span></span><br><span class="line">device = <span class="string">&quot;cuda&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span></span><br><span class="line">my_tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]], dtype=torch.float32,</span><br><span class="line">                         device= device, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(my_tensor)</span><br></pre></td></tr></table></figure><p>另外一种初始化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(size= (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">x = torch.zeros(size= (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">x = torch.rand(size= (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">x = torch.eye(size= (<span class="number">3</span>, <span class="number">3</span>)) <span class="comment">#单位矩阵</span></span><br><span class="line">x = torch.arange(start=<span class="number">0</span>, end=<span class="number">5</span>, step=<span class="number">1</span>) <span class="comment">#范围</span></span><br><span class="line">x = torch.linspace(start=<span class="number">0.1</span>, end=<span class="number">1</span>, steps=<span class="number">10</span>) <span class="comment">#0.1 - 1 以内</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p>linspace 和 arrange区别就是</p><p>arrange指明了首尾，步长，个数&#x3D;尾-首&#x2F;步长</p><p>linspace指明了首尾，个数，步长&#x3D;尾-首&#x2F;个数</p><p><img src="https://6c1ff8d.webp.li/20241229200247439.png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x = torch.empty(size= (<span class="number">1</span>, <span class="number">5</span>)).normal_(mean=<span class="number">0</span>, std = <span class="number">1</span>) <span class="comment">#正态分布</span></span><br><span class="line">x = torch.empty(size=(<span class="number">1</span>,<span class="number">5</span>)).uniform_(<span class="number">0</span>,<span class="number">1</span>) <span class="comment">#均匀分布，也就是随机</span></span><br><span class="line">                                          <span class="comment">#但是制定了范围</span></span><br><span class="line">x = torch.diag(torch.ones(<span class="number">3</span>))<span class="comment">#对角线</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Array To Tensor</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">np_array = np.zeros((<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">tensor = torch.from_numpy(np_array)</span><br><span class="line">np_array = tensor.numpy()</span><br></pre></td></tr></table></figure><h3 id="向量运算和比较"><a href="#向量运算和比较" class="headerlink" title="向量运算和比较"></a>向量运算和比较</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y = torch.tensor([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#Addtion</span></span><br><span class="line">z1 = torch.empty(<span class="number">3</span>)</span><br><span class="line">torch.add(x, y, out=z1)</span><br><span class="line"><span class="built_in">print</span>(z1)</span><br><span class="line"></span><br><span class="line">z2 = torch.add(x, y)</span><br><span class="line"></span><br><span class="line">z3 = x + y</span><br><span class="line"></span><br><span class="line"><span class="comment">#Subtraction</span></span><br><span class="line">z4 =  x - y</span><br><span class="line"><span class="built_in">print</span>(z4)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Division</span></span><br><span class="line">z5 = torch.true_divide(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment">#inplace operation</span></span><br><span class="line">t = torch.zeros(<span class="number">3</span>)</span><br><span class="line">t.add_(x)</span><br><span class="line">t += x <span class="comment">#t = t + x</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Expoentiation</span></span><br><span class="line">z6 = x.<span class="built_in">pow</span>(<span class="number">2</span>)</span><br><span class="line">z6 = x ** <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Simple comparison</span></span><br><span class="line">z7 = x &gt; <span class="number">0</span></span><br><span class="line">z7 = y &lt; <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Matrix Multiplacation</span></span><br><span class="line">x1 = torch.rand((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">x2 = torch.rand((<span class="number">5</span>, <span class="number">3</span>))</span><br><span class="line">x3 = torch.mm(x1, x2)</span><br><span class="line">x3 = x1.mm(x2) <span class="comment">#等价的</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Matrix exponentiation</span></span><br><span class="line">matrix_exp = torch.rand(<span class="number">5</span>, <span class="number">5</span>)</span><br><span class="line">matrix_exp.matrix_power(<span class="number">3</span>) <span class="comment">#矩阵n次幂</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#element wise mult</span></span><br><span class="line">z8 = x * y  </span><br><span class="line"><span class="built_in">print</span>(z8)</span><br><span class="line"></span><br><span class="line"><span class="comment">#dot product</span></span><br><span class="line">z9  = torch.dot(x, y)</span><br><span class="line"><span class="built_in">print</span>(z9)</span><br><span class="line"></span><br><span class="line"><span class="comment">#batch Matrix Multipliction</span></span><br><span class="line">batch = <span class="number">32</span></span><br><span class="line">n = <span class="number">10</span></span><br><span class="line">m = <span class="number">20</span></span><br><span class="line">p = <span class="number">30</span></span><br><span class="line"></span><br><span class="line">tensor1 = torch.rand((batch, n, m))</span><br><span class="line">tensor2 = torch.rand((batch, m, p))</span><br><span class="line">out_mm = torch.bmm(tensor1, tensor2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Broadcasting</span></span><br><span class="line"></span><br><span class="line">x1 = torch.rand((<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">x2 = torch.rand((<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">z = x1 - x2</span><br><span class="line">z = x1 ** x2</span><br><span class="line"><span class="comment">#也就是说，如果两个矩阵的维度不匹配，会自动扩展为相同列数和行数，再进行计算</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#other </span></span><br><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],[<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">sum_x = torch.<span class="built_in">sum</span>(x, dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(sum_x) </span><br><span class="line"><span class="comment">#具体来说，dim=0 指的是沿着第一维（即行的方向）进行操作，tensor([ 6, 15, 24])</span></span><br><span class="line"><span class="comment">#而 dim=1 则是沿着第二维（即列的方向）进行操作。tensor([12, 15, 18])</span></span><br><span class="line"></span><br><span class="line">values, indices = torch.<span class="built_in">max</span>(x, dim = <span class="number">0</span>) <span class="comment">#取每行最大值 可以直接x.max(dim = 0)</span></span><br><span class="line">values, indices = torch.<span class="built_in">min</span>(x, dim = <span class="number">1</span>) <span class="comment">#每列最小值</span></span><br><span class="line"></span><br><span class="line">abs_x = torch.<span class="built_in">abs</span>(x)</span><br><span class="line"></span><br><span class="line">z = torch.argmax(x, dim = <span class="number">0</span>)</span><br><span class="line">z = torch.argmin(x, dim = <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 函数功能返回值</span></span><br><span class="line"><span class="comment"># torch.max查找最大值如果没有 dim 参数，返回最大值本身。如果指定 dim，返回一个元组，其中第一个元素是最大值，第二个元素是最大值的索引</span></span><br><span class="line"><span class="comment"># torch.argmax查找最大值的索引返回最大值的索引（仅返回索引，不返回最大值本身）</span></span><br><span class="line"></span><br><span class="line">mean_x = torch.mean(x.<span class="built_in">float</span>(), dim = <span class="number">0</span>) <span class="comment">#平均值</span></span><br><span class="line">z = torch.eq(x, y) <span class="comment">#判断对应位置是否相等</span></span><br><span class="line"></span><br><span class="line">sorted_x, indices = torch.sort(x, dim = <span class="number">0</span>, descending = <span class="literal">False</span>) <span class="comment">#升序排序</span></span><br><span class="line"></span><br><span class="line">z = torch.clamp(x, <span class="built_in">min</span> = <span class="number">0</span>, <span class="built_in">max</span> = <span class="number">10</span>) <span class="comment">#将向量小于0的变为0，大于10的变为10</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="向量下标、取值、赋值"><a href="#向量下标、取值、赋值" class="headerlink" title="向量下标、取值、赋值"></a>向量下标、取值、赋值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">10</span></span><br><span class="line">features = <span class="number">25</span></span><br><span class="line">x = torch.rand((batch_size, features))</span><br><span class="line"><span class="comment">#x 将是一个 10 行 25 列的张量，形状是 (10, 25)</span></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">0</span>].shape) <span class="comment">#x[0, :]也就是行向量维数</span></span><br><span class="line"><span class="built_in">print</span>(x[:, <span class="number">0</span>].shape) <span class="comment">#提取所有行的第0列元素，也就是列向量的维数</span></span><br><span class="line"><span class="comment">#torch.Size([10, 25])</span></span><br><span class="line"><span class="comment"># torch.Size([25])</span></span><br><span class="line"><span class="comment"># torch.Size([10])</span></span><br><span class="line"><span class="built_in">print</span>(x[<span class="number">2</span>, <span class="number">0</span>:<span class="number">10</span>])</span><br><span class="line">x[<span class="number">0</span> , <span class="number">0</span>] = <span class="number">100</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#打印特定的行</span></span><br><span class="line">x = torch.arange(<span class="number">10</span>)</span><br><span class="line">indices = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">7</span>]</span><br><span class="line"><span class="built_in">print</span>(x[indices])</span><br><span class="line"></span><br><span class="line"><span class="comment">#Fancy index</span></span><br><span class="line">x = torch.rand((<span class="number">3</span>, <span class="number">5</span>))</span><br><span class="line">rows = torch.tensor([<span class="number">1</span>, <span class="number">0</span>])</span><br><span class="line">cols = torch.tensor([<span class="number">4</span>, <span class="number">0</span>])</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="built_in">print</span>(x[rows, cols]) <span class="comment">#rows的第一个元素和cols的第一个元素作为坐标</span></span><br><span class="line">                     <span class="comment">#同理对应第二个元素</span></span><br><span class="line">                     <span class="comment">#最终给出两个tensor</span></span><br><span class="line"><span class="comment"># tensor([[0.4546, 0.8152, 0.0296, 0.5971, 0.8530],</span></span><br><span class="line"><span class="comment">#         [0.3312, 0.4452, 0.5283, 0.3958, 0.0696],</span></span><br><span class="line"><span class="comment">#         [0.6694, 0.0788, 0.6363, 0.8018, 0.7811]])</span></span><br><span class="line"><span class="comment"># tensor([0.0696, 0.4546])</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#More advanced </span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x = torch.arange(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(x[(x &lt; <span class="number">2</span>) | (x &gt; <span class="number">8</span>)]) <span class="comment">#设定输出性质</span></span><br><span class="line"><span class="comment">#tensor([0, 1, 9])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x[x.remainder(<span class="number">2</span>) == <span class="number">0</span> ]) <span class="comment">#%2=0的数</span></span><br><span class="line"><span class="comment">#tensor([0, 2, 4, 6, 8])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.where(x &gt; <span class="number">5</span>, x, x*<span class="number">2</span>)) <span class="comment">#类似三目运算</span></span><br><span class="line">                                  <span class="comment">#大于5的依然是x</span></span><br><span class="line">                                  <span class="comment">#否则乘2</span></span><br><span class="line"><span class="comment">#tensor([ 0,  2,  4,  6,  8, 10,  6,  7,  8,  9])</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(torch.tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">6</span>]).unique()) <span class="comment">#取不重复出现的唯一值</span></span><br><span class="line"><span class="built_in">print</span>(x.ndimension()) <span class="comment">#维度</span></span><br><span class="line"><span class="built_in">print</span>(x.numel()) <span class="comment">#个数</span></span><br><span class="line"><span class="comment"># tensor([0, 1, 2, 5, 6])</span></span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 10</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="向量变换"><a href="#向量变换" class="headerlink" title="向量变换"></a>向量变换</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x = torch.<span class="built_in">range</span>(<span class="number">9</span>)</span><br><span class="line">x_33 = torch.view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">x_33 = torch.reshape(<span class="number">3</span>, <span class="number">3</span>)</span><br></pre></td></tr></table></figure><table><thead><tr><th>特性</th><th>view</th><th>reshape</th></tr></thead><tbody><tr><td>内存要求</td><td>原张量必须是连续的</td><td>无需连续内存，支持非连续张量</td></tr><tr><td>行为</td><td>如果内存不连续，抛出错误</td><td>会在必要时返回副本，确保连续性</td></tr><tr><td>返回值</td><td>返回视图（可能共享内存）</td><td>返回新张量（可能是副本）</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">y = x_33.t() <span class="comment">#转置</span></span><br><span class="line"><span class="comment">#需要注意的是，转置在内存上看来就是进行了跳跃</span></span><br><span class="line"><span class="comment">#所以y的元素的内存位置不再试连续的，所以使用y.view(9)将会出现错误</span></span><br><span class="line"><span class="comment">#解决方法，1.reshape；2.y.contiguous().view(9)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#tensors shape change</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x1 = torch.rand((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">x2 = torch.rand((<span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line"><span class="built_in">print</span>(torch.cat((x1, x2), dim = <span class="number">0</span>).shape) <span class="comment">#拼接两个矩阵，dim= 0 ,做行方向的拼接</span></span><br><span class="line"><span class="built_in">print</span>(torch.cat((x1, x2), dim = <span class="number">1</span>).shape) <span class="comment">#做列方向的拼接</span></span><br><span class="line"><span class="comment"># torch.Size([4, 5])</span></span><br><span class="line"><span class="comment"># torch.Size([2, 10])</span></span><br><span class="line"></span><br><span class="line">z = x1.view(-<span class="number">1</span>) <span class="comment">#直接将x1展开为一维的向量，不用自己计算应该是多大</span></span><br><span class="line"><span class="built_in">print</span>(z.shape)</span><br><span class="line"><span class="comment">#torch.Size([10])</span></span><br><span class="line"></span><br><span class="line">batch = <span class="number">64</span></span><br><span class="line">x = torch.rand((batch, <span class="number">2</span>, <span class="number">5</span>))</span><br><span class="line">z = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>) <span class="comment">#permute将x的维度（0,1,2）位置进行调换</span></span><br><span class="line">                       <span class="comment">#换为（0,2,1）</span></span><br><span class="line"><span class="built_in">print</span>(z.shape)</span><br><span class="line"><span class="comment">#torch.Size([64, 5, 2])</span></span><br><span class="line"></span><br><span class="line">x = torch.arange(<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(x.unsqueeze(<span class="number">0</span>).shape) <span class="comment">#unsqueeze在指定位置添加一个维度</span></span><br><span class="line"><span class="built_in">print</span>(x.unsqueeze(<span class="number">1</span>).shape) </span><br><span class="line"><span class="comment">#torch.Size([1, 10])</span></span><br><span class="line"><span class="comment">#torch.Size([10, 1])</span></span><br><span class="line"></span><br><span class="line">z = x.squeeze(<span class="number">1</span>) <span class="comment">#同理可以消除一个维度</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>请回答，我的2024</title>
      <link href="/read/%E8%AF%B7%E5%9B%9E%E7%AD%94%EF%BC%8C%E6%88%91%E7%9A%842024/"/>
      <url>/read/%E8%AF%B7%E5%9B%9E%E7%AD%94%EF%BC%8C%E6%88%91%E7%9A%842024/</url>
      
        <content type="html"><![CDATA[<p></style></p><h1 id="请回答，我的2024"><a href="#请回答，我的2024" class="headerlink" title="请回答，我的2024"></a>请回答，我的2024</h1><p>​我的2024在何时结束，是二十二号考研结束的落寞，或是24年最后一天晚后湖喧嚣的烟花，还是25年元旦岳麓山登峰的畅然。</p><p>​从一个无从考究的，某个年末开始，“年度xx”开始风靡，从”年度汉字“到”年度词语“，任何被冠以“年度”的名词似乎都可以引起人们对这一年进行“夹杂历史感的厚重的思考”，可惜，大多绞尽脑汁的检索只会带来悻悻的回答：“凡是过往，皆为序章，明年我一定要XX！”</p><p>​<strong>考研</strong>是贯穿我的2024的主题，从一月份寒假伊始决定考研，看了十来篇经验贴和择校就正式开始，记得三月份回校后那几天非常迷茫，对整体规划还是很模糊，择校更是痛苦纠结。随周数增加，三门专业课和其他选修课的压力也更大，10周之后，由于选的课大差不差，开启了排班轮换上课模式，课程作业、实验、期末缝缝补补地通过，大三就在带着荒诞的忙碌中结束了。</p><p>​但有时年度总结并不是给自己看的。例如，此刻，我的“年度主题”大概已经出炉——2025考研成绩。未知分数，可能也不需要分数，每当与相对熟络的人交际，考研一词总会从某一方口中蹦出。其中最关心你的肯定是你的学院或是辅导员，”名字很长的这位什么什么同学，我觉得你的2024你还行，但只是还行，如果你很棒的话你会被提名参加有聚光灯和摄像头的保研，所以你只是还行，为了维持我们不尴不尬的关系，现在我们送了考研礼包给你，再次祝愿这位名字很长的什么同学前程似锦。“当然以上都是调侃，事实上，感谢为我们付出的导员和同学。相信在未来几年里面，在考研名额减少，和各产业前景迷茫的未来，其他选择会回到平衡常态。虽然赚钱依旧是苦难重重，但是考研嘛，对小镇做题家来说算是个比较友好的归宿，除了考不上我找不到任何毛病。</p><p>​扔掉考研之后，我整个人自在了很多，下面是我的2024的年度总结。</p><h2 id="年度词——正确"><a href="#年度词——正确" class="headerlink" title="年度词——正确"></a>年度词——正确</h2><p>​今年我深深地困在了“正确”的语境中，困在“正确”的现实中。为什么考研？为什么找工作？为什么选这门课？晚上吃什么？等会玩什么？——说不清，我的回答只能用“说不清“来搪塞一切。我不需要”思想警察“来规训我，我自己就是自己的”思想警察“，这可能就是我会越来越冲突的一个内在矛盾，一方面我寻求个性的自由，一方面我又要主动回归集体性中，获得安全感。这次我不能再怪老师的压迫、家长的严苛了，我自己压抑掉独特性，成为一种正确。</p><p>我逐渐拒绝感受的存在，像机器一样统一情感的表达模式：提高学历是必须，进入体制是必须，找到工作是必须。长此以往，我变得越来越不真诚了，顺应大众是正确的，特立独行也是正确的，但自己的选择永远是是被裹挟着的正确的。很多观念迎合了我当下的感受，但是当这种感受越多越苛刻，也意味着我对别人也要如此。</p><p>​一月份，我接受了考研的正确，同时也接受了找工作的正确，更接受了我被最高的观念所裹挟的正确；八月份，我接受了被KTV的正确，也接受了独处的正确，人与人是互通的，非一方面的索取，而这种相互的施加是无形的，是无止境的，一方不满会不断扩大，一方不解也不断滋生。2024，我更多接受了互联网的正确，那些如影随形，稍纵即逝的“梗”恨不得在诞生之初就随着网线灌输到我脑中，并不断发酵爆发。所以我接受越来越多的正确，对别人变得越来越苛刻，这是我今年很难简单快乐的原因之一。</p><h2 id="年度困境——表达"><a href="#年度困境——表达" class="headerlink" title="年度困境——表达"></a>年度困境——表达</h2><p>​我相信我可以表达，但是越来越少的人愿意真实地表达了，不是别人不允许，而是我们自己不允许。既不能直抒胸臆，又做不到顺从别人，只能在一种微秒的别扭中支支吾吾，答非所问。我在害怕会承担一定的压力的情况下，批判着过时的思维模式，又拼命地往上面做，导致很多人在死磕这件事情，最后死磕成功的就算了，那些没成功的开始质疑自己这么多的努力最后换取不对等的回报，我的意义在哪里？更多时候执着的观念才是磨灭真诚表达的元凶。2024，除了每日同往的同学，我与他人的对话大大减少，大多时间被图书馆的躁动着的沉默吞噬掉，想说的话在自己内心不断消亡重生。但同很多应届生一样，我今天是恐慌的，一方面听说很多互联网大厂还在涨工资，一方面听说很多40岁左右的人被淘汰了，不仅互联网，很多岗位就是青春饭。不稳定感和不安全感好像从踏入社会的那一刻就开始不断挫败人的信心，抑制人的表达。也许困惑的部分才是常态，而那些过去的，蜂拥的，认同的价值也许才是上个时代留下来的问题。</p><h2 id="年度鸡汤——只有魔鬼才匆匆忙忙"><a href="#年度鸡汤——只有魔鬼才匆匆忙忙" class="headerlink" title="年度鸡汤——只有魔鬼才匆匆忙忙"></a>年度鸡汤——只有魔鬼才匆匆忙忙</h2><p>​从表面上看，我们的精神早就越出了中世纪的范畴，但其实不然，我们如同跳进湍急的河流，激流让我们身不由己地被推动，我们距离我们的根越来越远，距离我们的根越远我们就越狂躁，这已经成为了一种恶性循环。当我们和我们的根彻底断裂的时候，这种激流会永不停歇。</p><p>​同样失去了根，导致了人们不满足于各式各样的文明，我们的生活节奏越来越快，仿佛我们生活不是再当下，而是在未来，生活在虚无缥缈的未来的许诺中。可是我们的身体走的太快，灵魂却在原地踏步，我们日益暴躁易怒惶恐不安，为了派遣这种情绪，我们不得不用标新立异来救赎我们干涸的心灵 。我们生活的依靠不再是扎扎实实的现实，而是虚无缥缈的诺言，我们在生活中罔顾今天的光明，却专注未来的黑暗。我们心怀期待，认为黑暗是辉煌的前奏，可又自欺欺人，无视我们为了获取那些美好所付出的代价。可正如古代先贤的口头禅所说：“<strong>只有魔鬼才匆匆忙忙</strong>。”</p><p>​——节选《荣格自传》</p><h2 id="年度番外"><a href="#年度番外" class="headerlink" title="年度番外"></a>年度番外</h2><p>​在世界政治开始混乱，  民粹主义抬头，右翼思想来到，可能在未来的很长一段时间里，大家又会回到只关注本名族利益，而彼此筑起一个高墙，走向保守主义的时代（偷的考研政治），我，坚持了一年图书馆考研的日子。世界动荡和我半毛钱关系也没有！</p><p><img src="https://6c1ff8d.webp.li/qq_pic_merged_1735911873609.jpg"></p><p>​“时间不语，却回答了所有问题”，我的2024结束了，有的回答留在过去，有的问题绵延赓续，有的人不再陪伴，有的事烟消云散。庆幸的是！2025我青春依旧，风华正茂，有放下过去的释然，有开拓未来的信念，爱我所爱，行我所行，听从我心，无问西东！</p><p><img src="https://6c1ff8d.webp.li/IMG_20250101_121151.jpg" alt="元旦登峰！"></p><p>​2025-1-3</p><p>​凌晨</p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
